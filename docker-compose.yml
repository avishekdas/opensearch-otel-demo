# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0

version: "3.9"
x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"

volumes:
  opensearch-data1:
  opensearch-data2:
  prometheus_data:

networks:
  opensearch-otel-demo:

services:
  # ******************
  # Core Demo Services
  # ******************
  # Frontend
  frontend:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-frontend
    container_name: frontend
    build:
      context: ./
      dockerfile: ./src/frontend/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-frontend
    deploy:
      resources:
        limits:
          memory: 850M
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT}:${FRONTEND_PORT}"
    environment:
      - PORT=${FRONTEND_PORT}
      - FRONTEND_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - ENV_PLATFORM
      - OTEL_SERVICE_NAME=frontend
      - PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - WEB_OTEL_SERVICE_NAME=frontend-web
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging
    networks:
      - opensearch-otel-demo

  # Load Generator
  loadgenerator:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-loadgenerator
    container_name: load-generator
    build:
      context: ./
      dockerfile: ./src/loadgenerator/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-loadgenerator
    deploy:
      resources:
        limits:
          memory: 500M
    restart: unless-stopped
    ports:
      - "${LOCUST_WEB_PORT}:${LOCUST_WEB_PORT}"
    environment:
      - LOCUST_WEB_PORT
      - LOCUST_USERS
      - LOCUST_HOST
      - LOCUST_HEADLESS
      - LOCUST_AUTOSTART
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=loadgenerator
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    logging: *logging
    networks:
      - opensearch-otel-demo

  # Frontend Nginx Proxy service
  nginx:
    image: nginx:latest
    container_name: nginx
    volumes:
      - ./src/nginx-otel/default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - 90:90
    depends_on:
      - frontend
      - fluentbit
      - otelcol
      - loadgenerator
    links:
      - fluentbit
    logging:
      driver: "fluentd"
      options:
        fluentd-address: 127.0.0.1:24224
        tag: nginx.access
    networks:
      - opensearch-otel-demo

  # Fluent-bit logs shipper service
  fluentbit:
    container_name: fluentbit
    image: fluent/fluent-bit:latest
    volumes:
      - ./src/fluent-bit:/fluent-bit/etc
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    depends_on:
      - opensearch-dashboards
    networks:
      - opensearch-otel-demo

  # ******************
  # Dependent Services
  # ******************
  # Postgres used by Feature Flag service
  ffs_postgres:
    image: postgres:16.0
    container_name: postgres
    user: postgres
    deploy:
      resources:
        limits:
          memory: 500M
    restart: unless-stopped
    environment:
      - POSTGRES_USER=ffs
      - POSTGRES_DB=ffs
      - POSTGRES_PASSWORD=ffs
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d ffs -U ffs"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *logging
    networks:
      - opensearch-otel-demo

  # Kafka used by Checkout, Accounting, and Fraud Detection services
  kafka:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-kafka
    container_name: kafka
    build:
      context: ./
      dockerfile: ./src/kafka/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-kafka
    deploy:
      resources:
        limits:
          memory: 1500M
    restart: unless-stopped
    environment:
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=kafka
      - KAFKA_HEAP_OPTS=-Xmx200m -Xms200m
    healthcheck:
      test: nc -z kafka 9092
      start_period: 10s
      interval: 5s
      timeout: 10s
      retries: 10
    logging: *logging
    networks:
      - opensearch-otel-demo

  # Redis used by Cart service
  redis-cart:
    image: redis:7.2-alpine
    container_name: redis-cart
    user: redis
    deploy:
      resources:
        limits:
          memory: 250M
    restart: unless-stopped
    ports:
      - "${REDIS_PORT}"
    logging: *logging
    networks:
      - opensearch-otel-demo

  # ********************
  # Telemetry Components
  # ********************
  # Jaeger
  jaeger:
    image: jaegertracing/jaeger-collector:latest
    container_name: jaeger
    command:
      - "--metrics-backend=prometheus"
      - "--es.server-urls=https://opensearch-node1:9200"
      - "--es.tls.enabled=true"
    deploy:
      resources:
        limits:
          memory: 500M
    restart: unless-stopped
    ports:
      - "${JAEGER_SERVICE_PORT}" # Jaeger UI
      - "4317" # OTLP gRPC default port
      - "14269:14269"
      - "14268:14268"
      - "14267:14267"
      - "14250:14250"
      - "9411:9411"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - SPAN_STORAGE_TYPE=opensearch
      - ES_TAGS_AS_FIELDS_ALL=true
      - ES_USERNAME=admin
      - ES_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}
      - ES_TLS_SKIP_HOST_VERIFY=true
    logging: *logging
    depends_on:
      - opensearch-dashboards
    networks:
      - opensearch-otel-demo

  jaeger-agent:
    image: jaegertracing/jaeger-agent:latest
    container_name: jaeger-agent
    hostname: jaeger-agent
    command: ["--reporter.grpc.host-port=jaeger:14250"]
    ports:
      - "${GRAFANA_SERVICE_PORT}"
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
    restart: on-failure
    environment:
      - SPAN_STORAGE_TYPE=opensearch
    depends_on:
      - jaeger
    networks:
      - opensearch-otel-demo

  # Grafana
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    deploy:
      resources:
        limits:
          memory: 100M
    environment:
      - "GF_INSTALL_PLUGINS=grafana-opensearch-datasource"
    volumes:
      - ./src/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./src/grafana/provisioning/:/etc/grafana/provisioning/
    ports:
      - "${GRAFANA_SERVICE_PORT}:${GRAFANA_SERVICE_PORT}"
    logging: *logging
    networks:
      - opensearch-otel-demo

  # OpenTelemetry Collector
  otelcol:
    image: otel/opentelemetry-collector-contrib:0.93.0
    container_name: otel-col
    deploy:
      resources:
        limits:
          memory: 250M
    restart: unless-stopped
    command:
      [
        "--config=/etc/otelcol-config.yml",
        "--config=/etc/otelcol-config-extras.yml",
      ]
    volumes:
      - ./src/otelcollector/otelcol-config.yml:/etc/otelcol-config.yml
      - ./src/otelcollector/otelcol-config-extras.yml:/etc/otelcol-config-extras.yml
    ports:
      - "4317" # OTLP over gRPC receiver
      - "4318:4318" # OTLP over HTTP receiver
      - "13133:13133" # health check port
      - "9464" # Prometheus exporter
      - "8888" # metrics endpoint
    depends_on:
      - jaeger-agent
      - data-prepper
    logging: *logging
    environment:
      - ENVOY_PORT
    networks:
      - opensearch-otel-demo

  # Prometheus
  prometheus:
    image: quay.io/prometheus/prometheus:v2.47.2
    container_name: prometheus
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
      - --enable-feature=otlp-write-receiver
    volumes:
      - prometheus_data:/usr/share/prometheus/data
      - ./src/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
    deploy:
      resources:
        limits:
          memory: 500M
    ports:
      - "${PROMETHEUS_SERVICE_PORT}:${PROMETHEUS_SERVICE_PORT}"
    logging: *logging
    networks:
      - opensearch-otel-demo


  opensearch-node1:
    image: opensearchproject/opensearch:${OPENSEARCH_VERSION}
    container_name: opensearch-node1
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - plugins.query.datasources.encryption.masterkey=8e3f206ea7c07cc1bfc5cf40
      - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM
      - "OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536 # maximum number of open files for the OpenSearch user, set to at least 65536 on modern systems
        hard: 65536
    volumes:
      - opensearch-data1:/usr/share/opensearch/data/${OPENSEARCH_VERSION}
    ports:
      - 9200:9200
      - 9600:9600 # required for Performance Analyzer
    networks:
      - opensearch-otel-demo
  
  opensearch-node2:
    image: opensearchproject/opensearch:${OPENSEARCH_VERSION}
    container_name: opensearch-node2
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node2
      - discovery.seed_hosts=opensearch-node1,opensearch-node2
      - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2
      - bootstrap.memory_lock=true
      - plugins.query.datasources.encryption.masterkey=8e3f206ea7c07cc1bfc5cf40
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data2:/usr/share/opensearch/data/${OPENSEARCH_VERSION}
    networks:
      - opensearch-otel-demo

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:${OPENSEARCH_VERSION}
    container_name: opensearch-dashboards
    ports:
      - 5601:5601
    expose:
      - "5601"
    environment:
      - 'OPENSEARCH_HOSTS=["https://opensearch-node1:9200","https://opensearch-node2:9200"]'
    depends_on:
      - opensearch-node1
      - opensearch-node2
    networks:
      - opensearch-otel-demo
    volumes:
      - ./src/dashboards/config/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml

  # OpenSearch store - dashboard
  data-prepper:
    platform: linux/amd64
    image: opensearchproject/data-prepper:${DATA_PREP_VERSION}
    container_name: dataprepper
    volumes:
      - /data/service-map/
      - ./src/dataprepper/templates/ss4o_metrics.json:/usr/share/data-prepper/templates/ss4o_metrics.json
      - ./src/dataprepper/templates/ss4o_logs.json:/usr/share/data-prepper/templates/ss4o_logs.json
      - ./src/dataprepper/templates/ss4o_traces.json:/usr/share/data-prepper/templates/ss4o_traces.json
      - ./src/dataprepper/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml
      - ./src/dataprepper/data-prepper-config.yaml:/usr/share/data-prepper/config/data-prepper-config.yaml
    ports:
      - "21890:21890"
      - "21891:21891"
      - "21892:21892"
    expose:
      - "21890"
      - "21891"
      - "21892"
    logging: *logging
    depends_on:
      - opensearch-dashboards
    networks:
      - opensearch-otel-demo

  # *****
  # Tests
  # *****
  tracetest-server:
    image: kubeshop/tracetest:v0.14.5
    platform: linux/amd64
    container_name: tracetest-server
    profiles:
      - tests
    volumes:
      - type: bind
        source: ./test/tracetesting/tracetest-config.yaml
        target: /app/tracetest.yaml
      - type: bind
        source: ./test/tracetesting/tracetest-provision.yaml
        target: /app/provision.yaml
    command: --provisioning-file /app/provision.yaml
    ports:
      - 11633:11633
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      tracetest-postgres:
        condition: service_healthy
      otelcol:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--spider", "localhost:11633"]
      interval: 1s
      timeout: 3s
      retries: 60
    networks:
      - opensearch-otel-demo

  tracetest-postgres:
    image: postgres:16.0
    container_name: tracetest-postgres
    profiles:
      - tests
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
    healthcheck:
      test: pg_isready -U "$$POSTGRES_USER" -d "$$POSTGRES_DB"
      interval: 1s
      timeout: 5s
      retries: 60
    ports:
      - 5432
    networks:
      - opensearch-otel-demo
